import numpy as np
import matplotlib.pyplot as plt
import scipy
from scipy.special import softmax
import pandas as pd
from sklearn.model_selection import train_test_split

def activation(z):
    return softmax(z)

def initialize_parameters(input_layer, hidden_layer, output_layer):
    # initialize 1st layer output and input with random values
    W1 = np.random.randn(hidden_layer, input_layer) * 0.01
    # initialize 1st layer output bias
    b1 = np.zeros((hidden_layer, 1))
    # initialize 2nd layer output and input with random values
    W2 = np.random.randn(output_layer, hidden_layer) * 0.01
    # initialize 2nd layer output bias
    b2 = np.zeros((output_layer, 1))

    parameters = {"W1": W1,
                  "b1": b1,
                  "W2": W2,
                  "b2": b2}

    return parameters


def forward_propagation(X, parameters):
    # Retrieve each parameter from the dictionary "parameters"
    W1 = parameters["W1"]
    b1 = parameters["b1"]
    W2 = parameters["W2"]
    b2 = parameters["b2"]

    # Implementing Forward Propagation to calculate A2 probabilities
    Z1 = np.dot(W1, X) + b1
    A1 = np.tanh(Z1)
    Z2 = np.dot(W2, A1) + b2
    A2 = activation(Z2)

    # Values needed in the backpropagation are stored in "cache"
    cache = {"Z1": Z1,
             "A1": A1,
             "Z2": Z2,
             "A2": A2}

    return A2, cache


def compute_cost(A2, Y, parameters):
    # number of example
    m = Y.shape[1]

    # Compute the binary cross-entropy cost
    logprobs = np.multiply(np.log(A2), Y) + np.multiply(np.log(1 - A2), (1 - Y))
    cost = -1 / m * np.sum(logprobs)

    # makes sure cost is in dimension we expect, E.g., turns [[51]] into 51
    cost = np.squeeze(cost)

    return cost


def backward_propagation(parameters, cache, X, Y):
    # number of example
    m = X.shape[1]

    # Retrieve W1 and W2 from the "parameters" dictionary
    W1 = parameters["W1"]
    W2 = parameters["W2"]

    # Retrieve A1 and A2 from "cache" dictionary
    A1 = cache["A1"]
    A2 = cache["A2"]

    # Backward propagation for dW1, db1, dW2, db2
    dZ2 = A2 - Y
    dW2 = 1. / m * np.dot(dZ2, A1.T)
    db2 = 1. / m * np.sum(dZ2, axis=1, keepdims=True)
    dZ1 = np.dot(W2.T, dZ2) * (1 - np.power(A1, 2))
    dW1 = 1. / m * np.dot(dZ1, X.T)
    db1 = 1. / m * np.sum(dZ1, axis=1, keepdims=True)

    grads = {"dW1": dW1,
             "db1": db1,
             "dW2": dW2,
             "db2": db2}

    return grads


def update_parameters(parameters, grads, learning_rate=0.1):
    # Retrieve each parameter from "parameters" dictionary
    W1 = parameters["W1"]
    W2 = parameters["W2"]
    b1 = parameters["b1"]
    b2 = parameters["b2"]

    # Retrieve each gradient from the "grads" dictionary
    dW1 = grads["dW1"]
    db1 = grads["db1"]
    dW2 = grads["dW2"]
    db2 = grads["db2"]

    # Update rule for each parameter
    W1 = W1 - dW1 * learning_rate
    b1 = b1 - db1 * learning_rate
    W2 = W2 - dW2 * learning_rate
    b2 = b2 - db2 * learning_rate

    parameters = {"W1": W1,
                  "b1": b1,
                  "W2": W2,
                  "b2": b2}

    return parameters


def predict(parameters, X):
    # Computes probabilities using forward propagation
    Y_prediction = np.zeros((1, X.shape[1]))
    A2, cache = forward_propagation(X, parameters)

    for i in range(A2.shape[1]):
        # Convert probabilities A[0,i] to actual predictions p[0,i]
        if A2[0, i] > 0.5:
            Y_prediction[[0], [i]] = 1
        else:
            Y_prediction[[0], [i]] = 0

    return Y_prediction

def nn_model(X_train, Y_train, X_test, Y_test, n_h, num_iterations=1000, learning_rate=0.05, print_cost=False):
    n_x = X_train.shape[0]
    n_y = Y_train.shape[0]
    n_xt = X_test.shape[0]
    n_yt = Y_test.shape[0]

    # Initialize parameters with inputs: "n_x, n_h, n_y"
    parameters = initialize_parameters(n_x, n_h, n_y)
    pt = initialize_parameters(n_xt, n_h, n_yt)

    # Retrieve W1, b1, W2, b2
    W1 = parameters["W1"]
    W2 = parameters["W2"]
    b1 = parameters["b1"]
    b2 = parameters["b2"]

    costs = []
    for i in range(0, num_iterations):
        # Forward propagation. Inputs: "X, parameters". Outputs: "A2, cache".
        A2, cache = forward_propagation(X_train, parameters)

        # Cost function. Inputs: "A2, Y, parameters". Outputs: "cost".
        cost = compute_cost(A2, Y_train, parameters)

        # Backpropagation. Inputs: "parameters, cache, X, Y". Outputs: "grads".
        grads = backward_propagation(parameters, cache, X_train, Y_train)

        # Gradient descent parameter update. Inputs: "parameters, grads". Outputs: "parameters".
        parameters = update_parameters(parameters, grads, learning_rate)

        # Print the cost every 200 iterations
        if print_cost and i % 200 == 0:
            print("Cost after iteration %i: %f" % (i, cost))

        # Record the cost
        if i % 100 == 0:
            costs.append(cost)

    # Predict test/train set examples
    Y_prediction_train = predict(parameters, X_train)
    Y_prediction_test = predict(pt, X_test)


    # Print train/test Errors
    print("train accuracy: {} %".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))
    print("test accuracy: {} %".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))

    parameters.update({"costs": costs, "n_h": n_h})
    return parameters

dataset = pd.read_csv("Landslide_book.csv")
X = np.array(dataset['Apparent Coefficeint of Friction(H/R)'])
Y = np.array(dataset['Classifier'])
X = X.reshape((72,1))
Y = Y.reshape((72,1))
data = np.concatenate((X,Y), axis=1)
#print(data)
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25)
nn_model(X_train,Y_train,X_test,Y_test,10,200,0.02,True)
